{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:\\\\gab\\\\train.csv',\n",
    "                   sep=',', \n",
    "                   header=None, \n",
    "                   names=['article', 'label'],\n",
    "                   encoding='ISO-8859-1')\n",
    "df1 = pd.read_csv('C:\\\\gab\\\\test.csv',\n",
    "                   sep=',', \n",
    "                   header=None, \n",
    "                   names=['article', 'label'],\n",
    "                   encoding='ISO-8859-1')\n",
    "df2 = pd.read_csv('C:\\\\gab\\\\dev.csv',\n",
    "                   sep=',', \n",
    "                   header=None, \n",
    "                   names=['article', 'label'],\n",
    "                   encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19093, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4133, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4052, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's wrong with that? It's History you know....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Switzerland, I'm guessing ? if so, an outlier ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are they selling ? Are? they trying to? m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://bit.do/b7ndm: Dr. Paul Powers is back i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@support? the prompt for browser notifications...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I really hope against all hope, that Sessions ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italy: Mass Deportation Of Migrants May Start ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A Sith Lord's level of mental illness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I don't endorse any of the links on this site,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Maybe it's me being a bit jaded because I'm in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  label\n",
       "0  What's wrong with that? It's History you know....      0\n",
       "1  Switzerland, I'm guessing ? if so, an outlier ...      0\n",
       "2  What are they selling ? Are? they trying to? m...      0\n",
       "3  http://bit.do/b7ndm: Dr. Paul Powers is back i...      0\n",
       "4  @support? the prompt for browser notifications...      0\n",
       "5  I really hope against all hope, that Sessions ...      0\n",
       "6  Italy: Mass Deportation Of Migrants May Start ...      0\n",
       "7           A Sith Lord's level of mental illness...      0\n",
       "8  I don't endorse any of the links on this site,...      0\n",
       "9  Maybe it's me being a bit jaded because I'm in...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please go to Dane's website at www.geoengineer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anybody else sick of online product launch vid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hold on #GabFam I am just getting warm, blowin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Democrat Party exists for the media's globalis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Relegion of open and 'lawful' child sexual abu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.bitchute.com/video/not1pbGQA9uE/  ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Can't wait to see former Tory MP's crying woe ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>There's a phrase being used right now that I t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://rickyrescue.com/florida-state-fire-cer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Look, gun grabbers. I have wasted a lot of bre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  label\n",
       "0  Please go to Dane's website at www.geoengineer...      0\n",
       "1  Anybody else sick of online product launch vid...      1\n",
       "2  Hold on #GabFam I am just getting warm, blowin...      0\n",
       "3  Democrat Party exists for the media's globalis...      0\n",
       "4  Relegion of open and 'lawful' child sexual abu...      1\n",
       "5  https://www.bitchute.com/video/not1pbGQA9uE/  ...      0\n",
       "6  Can't wait to see former Tory MP's crying woe ...      0\n",
       "7  There's a phrase being used right now that I t...      0\n",
       "8  https://rickyrescue.com/florida-state-fire-cer...      0\n",
       "9  Look, gun grabbers. I have wasted a lot of bre...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lewis's brother, Charles, King of Italy (also ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hope the person who invented wallpaper is ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Same reason we never told you about the \"Holoc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anamorph - A game of perspective. The fun of a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes he should he is working against the Presid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What does ¡®acting like Californians¡¯ mean?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I guess I'll do a small rundown from the stuff...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>He is still ignoring the communists... except ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Radio Derb Transcript Up: Explaining Discharge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>found it.    http://www.whsports.se    channel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  label\n",
       "0  Lewis's brother, Charles, King of Italy (also ...      0\n",
       "1  I hope the person who invented wallpaper is ex...      0\n",
       "2  Same reason we never told you about the \"Holoc...      0\n",
       "3  Anamorph - A game of perspective. The fun of a...      0\n",
       "4  Yes he should he is working against the Presid...      0\n",
       "5       What does ¡®acting like Californians¡¯ mean?      0\n",
       "6  I guess I'll do a small rundown from the stuff...      0\n",
       "7  He is still ignoring the communists... except ...      0\n",
       "8  Radio Derb Transcript Up: Explaining Discharge...      0\n",
       "9  found it.    http://www.whsports.se    channel...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0  16898\n",
       "1   2195"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_counts = pd.DataFrame(df['label'].value_counts())\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0   3658\n",
       "1    475"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = pd.DataFrame(df1['label'].value_counts())\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0   3577\n",
       "1    475"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = pd.DataFrame(df2['label'].value_counts())\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_values = list(label_counts.index)\n",
    "order = list(pd.DataFrame(df['label'].value_counts()).index)\n",
    "label_values = [l for _,l in sorted(zip(order, label_values))]\n",
    "\n",
    "label_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_values = list(label_counts.index)\n",
    "order = list(pd.DataFrame(df1['label'].value_counts()).index)\n",
    "label_values = [l for _,l in sorted(zip(order, label_values))]\n",
    "\n",
    "label_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_values = list(label_counts.index)\n",
    "order = list(pd.DataFrame(df2['label'].value_counts()).index)\n",
    "label_values = [l for _,l in sorted(zip(order, label_values))]\n",
    "\n",
    "label_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['article'].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts1 = df1['article'].values\n",
    "labels1 = df1['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2 = df2['article'].values\n",
    "labels2 = df2['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_lengths = [len(texts[i].split()) for i in range(len(texts))]\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "text_lengths1 = [len(texts1[i].split()) for i in range(len(texts1))]\n",
    "print(min(text_lengths1))\n",
    "print(max(text_lengths1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "815\n"
     ]
    }
   ],
   "source": [
    "text_lengths2 = [len(texts2[i].split()) for i in range(len(texts2))]\n",
    "print(min(text_lengths2))\n",
    "print(max(text_lengths2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for i in range(len(text_lengths)) if text_lengths[i] >= 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for i in range(len(text_lengths1)) if text_lengths1[i] >= 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for i in range(len(text_lengths2)) if text_lengths2[i] >= 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  What's wrong with that? It's History you know. Mary Beard told me so and she has a vagina from Oxford. \n",
      "\n",
      "Tokenized Text:  ['what', \"'\", 's', 'wrong', 'with', 'that', '?', 'it', \"'\", 's', 'history', 'you', 'know', '.', 'mary', 'beard', 'told', 'me', 'so', 'and', 'she', 'has', 'a', 'va', '##gina', 'from', 'oxford', '.'] \n",
      "\n",
      "Token IDs:  [2054, 1005, 1055, 3308, 2007, 2008, 1029, 2009, 1005, 1055, 2381, 2017, 2113, 1012, 2984, 10154, 2409, 2033, 2061, 1998, 2016, 2038, 1037, 12436, 20876, 2013, 4345, 1012]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "print('Original Text: ', texts[0], '\\n')\n",
    "print('Tokenized Text: ', tokenizer.tokenize(texts[0]), '\\n')\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  Please go to Dane's website at www.geoengineering.org and educate yourselves on this global crime, it's your children's, and their children's lives at risk. Thank you.  \n",
      "\n",
      "Tokenized Text:  ['please', 'go', 'to', 'dane', \"'\", 's', 'website', 'at', 'www', '.', 'geo', '##eng', '##ine', '##ering', '.', 'org', 'and', 'educate', 'yourselves', 'on', 'this', 'global', 'crime', ',', 'it', \"'\", 's', 'your', 'children', \"'\", 's', ',', 'and', 'their', 'children', \"'\", 's', 'lives', 'at', 'risk', '.', 'thank', 'you', '.'] \n",
      "\n",
      "Token IDs:  [3531, 2175, 2000, 14569, 1005, 1055, 4037, 2012, 7479, 1012, 20248, 13159, 3170, 7999, 1012, 8917, 1998, 16957, 25035, 2006, 2023, 3795, 4126, 1010, 2009, 1005, 1055, 2115, 2336, 1005, 1055, 1010, 1998, 2037, 2336, 1005, 1055, 3268, 2012, 3891, 1012, 4067, 2017, 1012]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "print('Original Text: ', texts1[0], '\\n')\n",
    "print('Tokenized Text: ', tokenizer.tokenize(texts1[0]), '\\n')\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts1[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  Lewis's brother, Charles, King of Italy (also known as Charles the Fat, as he was an obese slob) sat by idly while all this was happening and did nothing.? The Pope gave him the title of Emperor, hoping he this would spur him to action, but still he did nothing.? When Lewis died, Charles became ruler over his lands as well.? Messengers hastened to him, begging him to come save his people from d... \n",
      "\n",
      "Tokenized Text:  ['lewis', \"'\", 's', 'brother', ',', 'charles', ',', 'king', 'of', 'italy', '(', 'also', 'known', 'as', 'charles', 'the', 'fat', ',', 'as', 'he', 'was', 'an', 'obe', '##se', 'sl', '##ob', ')', 'sat', 'by', 'id', '##ly', 'while', 'all', 'this', 'was', 'happening', 'and', 'did', 'nothing', '.', '?', 'the', 'pope', 'gave', 'him', 'the', 'title', 'of', 'emperor', ',', 'hoping', 'he', 'this', 'would', 'spur', 'him', 'to', 'action', ',', 'but', 'still', 'he', 'did', 'nothing', '.', '?', 'when', 'lewis', 'died', ',', 'charles', 'became', 'ruler', 'over', 'his', 'lands', 'as', 'well', '.', '?', 'messengers', 'haste', '##ned', 'to', 'him', ',', 'begging', 'him', 'to', 'come', 'save', 'his', 'people', 'from', 'd', '.', '.', '.'] \n",
      "\n",
      "Token IDs:  [4572, 1005, 1055, 2567, 1010, 2798, 1010, 2332, 1997, 3304, 1006, 2036, 2124, 2004, 2798, 1996, 6638, 1010, 2004, 2002, 2001, 2019, 15578, 3366, 22889, 16429, 1007, 2938, 2011, 8909, 2135, 2096, 2035, 2023, 2001, 6230, 1998, 2106, 2498, 1012, 1029, 1996, 4831, 2435, 2032, 1996, 2516, 1997, 3750, 1010, 5327, 2002, 2023, 2052, 12996, 2032, 2000, 2895, 1010, 2021, 2145, 2002, 2106, 2498, 1012, 1029, 2043, 4572, 2351, 1010, 2798, 2150, 7786, 2058, 2010, 4915, 2004, 2092, 1012, 1029, 28938, 24748, 7228, 2000, 2032, 1010, 12858, 2032, 2000, 2272, 3828, 2010, 2111, 2013, 1040, 1012, 1012, 1012]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "print('Original Text: ', texts2[0], '\\n')\n",
    "print('Tokenized Text: ', tokenizer.tokenize(texts2[0]), '\\n')\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2054,\n",
       " 1005,\n",
       " 1055,\n",
       " 3308,\n",
       " 2007,\n",
       " 2008,\n",
       " 1029,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2381,\n",
       " 2017,\n",
       " 2113,\n",
       " 1012,\n",
       " 2984,\n",
       " 10154,\n",
       " 2409,\n",
       " 2033,\n",
       " 2061,\n",
       " 1998,\n",
       " 2016,\n",
       " 2038,\n",
       " 1037,\n",
       " 12436,\n",
       " 20876,\n",
       " 2013,\n",
       " 4345,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "text_ids = [tokenizer.encode(text, max_length=100, pad_to_max_length=True) for text in texts]\n",
    "\n",
    "text_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 3531,\n",
       " 2175,\n",
       " 2000,\n",
       " 14569,\n",
       " 1005,\n",
       " 1055,\n",
       " 4037,\n",
       " 2012,\n",
       " 7479,\n",
       " 1012,\n",
       " 20248,\n",
       " 13159,\n",
       " 3170,\n",
       " 7999,\n",
       " 1012,\n",
       " 8917,\n",
       " 1998,\n",
       " 16957,\n",
       " 25035,\n",
       " 2006,\n",
       " 2023,\n",
       " 3795,\n",
       " 4126,\n",
       " 1010,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2115,\n",
       " 2336,\n",
       " 1005,\n",
       " 1055,\n",
       " 1010,\n",
       " 1998,\n",
       " 2037,\n",
       " 2336,\n",
       " 1005,\n",
       " 1055,\n",
       " 3268,\n",
       " 2012,\n",
       " 3891,\n",
       " 1012,\n",
       " 4067,\n",
       " 2017,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids1 = [tokenizer.encode(text, max_length=100, pad_to_max_length=True) for text in texts1]\n",
    "\n",
    "text_ids1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 4572,\n",
       " 1005,\n",
       " 1055,\n",
       " 2567,\n",
       " 1010,\n",
       " 2798,\n",
       " 1010,\n",
       " 2332,\n",
       " 1997,\n",
       " 3304,\n",
       " 1006,\n",
       " 2036,\n",
       " 2124,\n",
       " 2004,\n",
       " 2798,\n",
       " 1996,\n",
       " 6638,\n",
       " 1010,\n",
       " 2004,\n",
       " 2002,\n",
       " 2001,\n",
       " 2019,\n",
       " 15578,\n",
       " 3366,\n",
       " 22889,\n",
       " 16429,\n",
       " 1007,\n",
       " 2938,\n",
       " 2011,\n",
       " 8909,\n",
       " 2135,\n",
       " 2096,\n",
       " 2035,\n",
       " 2023,\n",
       " 2001,\n",
       " 6230,\n",
       " 1998,\n",
       " 2106,\n",
       " 2498,\n",
       " 1012,\n",
       " 1029,\n",
       " 1996,\n",
       " 4831,\n",
       " 2435,\n",
       " 2032,\n",
       " 1996,\n",
       " 2516,\n",
       " 1997,\n",
       " 3750,\n",
       " 1010,\n",
       " 5327,\n",
       " 2002,\n",
       " 2023,\n",
       " 2052,\n",
       " 12996,\n",
       " 2032,\n",
       " 2000,\n",
       " 2895,\n",
       " 1010,\n",
       " 2021,\n",
       " 2145,\n",
       " 2002,\n",
       " 2106,\n",
       " 2498,\n",
       " 1012,\n",
       " 1029,\n",
       " 2043,\n",
       " 4572,\n",
       " 2351,\n",
       " 1010,\n",
       " 2798,\n",
       " 2150,\n",
       " 7786,\n",
       " 2058,\n",
       " 2010,\n",
       " 4915,\n",
       " 2004,\n",
       " 2092,\n",
       " 1012,\n",
       " 1029,\n",
       " 28938,\n",
       " 24748,\n",
       " 7228,\n",
       " 2000,\n",
       " 2032,\n",
       " 1010,\n",
       " 12858,\n",
       " 2032,\n",
       " 2000,\n",
       " 2272,\n",
       " 3828,\n",
       " 2010,\n",
       " 2111,\n",
       " 2013,\n",
       " 1040,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids2 = [tokenizer.encode(text, max_length=100, pad_to_max_length=True) for text in texts2]\n",
    "\n",
    "text_ids2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "smote = SMOTE(random_state=777,k_neighbors=5)\n",
    "x_smote,y_smote = smote.fit_sample(text_ids,labels)\n",
    "\n",
    "\n",
    "\n",
    "#print(X_smote,y_smote )\n",
    "\n",
    "print(y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_ids_lengths = [len(x_smote[i]) for i in range(len(x_smote))]\n",
    "print(min(text_ids_lengths))\n",
    "print(max(text_ids_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "text_ids_lengths1 = [len(text_ids1[i]) for i in range(len(text_ids1))]\n",
    "print(min(text_ids_lengths1))\n",
    "print(max(text_ids_lengths1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "text_ids_lengths2 = [len(text_ids2[i]) for i in range(len(text_ids2))]\n",
    "print(min(text_ids_lengths2))\n",
    "print(max(text_ids_lengths2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_masks = []\n",
    "for ids in x_smote:\n",
    "    masks = [int(id > 0) for id in ids]\n",
    "    att_masks.append(masks)\n",
    "    \n",
    "att_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_masks1 = []\n",
    "for ids in text_ids1:\n",
    "    masks = [int(id > 0) for id in ids]\n",
    "    att_masks1.append(masks)\n",
    "    \n",
    "att_masks1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_masks2 = []\n",
    "for ids in text_ids2:\n",
    "    masks = [int(id > 0) for id in ids]\n",
    "    att_masks2.append(masks)\n",
    "    \n",
    "att_masks2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train_x, test_val_x, train_y, test_val_y = train_test_split(text_ids, labels, random_state=111, test_size=0.2)\n",
    "#train_m, test_val_m = train_test_split(att_masks, random_state=111, test_size=0.2)\n",
    "\n",
    "#test_x, val_x, test_y, val_y = train_test_split(test_val_x, test_val_y, random_state=111, test_size=0.5)\n",
    "#test_m, val_m = train_test_split(test_val_m, random_state=111, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x_smote\n",
    "train_y = y_smote\n",
    "train_m = att_masks\n",
    "\n",
    "val_x = text_ids1\n",
    "val_y = labels1\n",
    "val_m = att_masks1\n",
    "\n",
    "test_x = text_ids2\n",
    "test_y = labels2\n",
    "test_m = att_masks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33796, 100])\n",
      "torch.Size([4052, 100])\n",
      "torch.Size([4133, 100])\n",
      "torch.Size([33796])\n",
      "torch.Size([4052])\n",
      "torch.Size([4133])\n",
      "torch.Size([33796, 100])\n",
      "torch.Size([4052, 100])\n",
      "torch.Size([4133, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_x = torch.tensor(x_smote)\n",
    "test_x = torch.tensor(test_x)\n",
    "val_x = torch.tensor(val_x)\n",
    "train_y = torch.tensor(y_smote)\n",
    "test_y = torch.tensor(test_y)\n",
    "val_y = torch.tensor(val_y)\n",
    "train_m = torch.tensor(train_m)\n",
    "test_m = torch.tensor(test_m)\n",
    "val_m = torch.tensor(val_m)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(val_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)\n",
    "print(val_y.shape)\n",
    "print(train_m.shape)\n",
    "print(test_m.shape)\n",
    "print(val_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_x, train_m, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_x, val_m, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig\n",
    "\n",
    "num_labels = len(set(labels))\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels,\n",
    "                                                            output_attentions=False, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 66955010 \n",
      " DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of trainable parameters:', count_parameters(model), '\\n', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distilbert.embeddings.word_embeddings.weight',\n",
       " 'distilbert.embeddings.position_embeddings.weight',\n",
       " 'distilbert.embeddings.LayerNorm.weight',\n",
       " 'distilbert.embeddings.LayerNorm.bias',\n",
       " 'distilbert.transformer.layer.0.attention.q_lin.weight',\n",
       " 'distilbert.transformer.layer.0.attention.q_lin.bias',\n",
       " 'distilbert.transformer.layer.0.attention.k_lin.weight',\n",
       " 'distilbert.transformer.layer.0.attention.k_lin.bias',\n",
       " 'distilbert.transformer.layer.0.attention.v_lin.weight',\n",
       " 'distilbert.transformer.layer.0.attention.v_lin.bias',\n",
       " 'distilbert.transformer.layer.0.attention.out_lin.weight',\n",
       " 'distilbert.transformer.layer.0.attention.out_lin.bias',\n",
       " 'distilbert.transformer.layer.0.sa_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.0.sa_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.0.ffn.lin1.weight',\n",
       " 'distilbert.transformer.layer.0.ffn.lin1.bias',\n",
       " 'distilbert.transformer.layer.0.ffn.lin2.weight',\n",
       " 'distilbert.transformer.layer.0.ffn.lin2.bias',\n",
       " 'distilbert.transformer.layer.0.output_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.0.output_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.1.attention.q_lin.weight',\n",
       " 'distilbert.transformer.layer.1.attention.q_lin.bias',\n",
       " 'distilbert.transformer.layer.1.attention.k_lin.weight',\n",
       " 'distilbert.transformer.layer.1.attention.k_lin.bias',\n",
       " 'distilbert.transformer.layer.1.attention.v_lin.weight',\n",
       " 'distilbert.transformer.layer.1.attention.v_lin.bias',\n",
       " 'distilbert.transformer.layer.1.attention.out_lin.weight',\n",
       " 'distilbert.transformer.layer.1.attention.out_lin.bias',\n",
       " 'distilbert.transformer.layer.1.sa_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.1.sa_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.1.ffn.lin1.weight',\n",
       " 'distilbert.transformer.layer.1.ffn.lin1.bias',\n",
       " 'distilbert.transformer.layer.1.ffn.lin2.weight',\n",
       " 'distilbert.transformer.layer.1.ffn.lin2.bias',\n",
       " 'distilbert.transformer.layer.1.output_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.1.output_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.2.attention.q_lin.weight',\n",
       " 'distilbert.transformer.layer.2.attention.q_lin.bias',\n",
       " 'distilbert.transformer.layer.2.attention.k_lin.weight',\n",
       " 'distilbert.transformer.layer.2.attention.k_lin.bias',\n",
       " 'distilbert.transformer.layer.2.attention.v_lin.weight',\n",
       " 'distilbert.transformer.layer.2.attention.v_lin.bias',\n",
       " 'distilbert.transformer.layer.2.attention.out_lin.weight',\n",
       " 'distilbert.transformer.layer.2.attention.out_lin.bias',\n",
       " 'distilbert.transformer.layer.2.sa_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.2.sa_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.2.ffn.lin1.weight',\n",
       " 'distilbert.transformer.layer.2.ffn.lin1.bias',\n",
       " 'distilbert.transformer.layer.2.ffn.lin2.weight',\n",
       " 'distilbert.transformer.layer.2.ffn.lin2.bias',\n",
       " 'distilbert.transformer.layer.2.output_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.2.output_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.3.attention.q_lin.weight',\n",
       " 'distilbert.transformer.layer.3.attention.q_lin.bias',\n",
       " 'distilbert.transformer.layer.3.attention.k_lin.weight',\n",
       " 'distilbert.transformer.layer.3.attention.k_lin.bias',\n",
       " 'distilbert.transformer.layer.3.attention.v_lin.weight',\n",
       " 'distilbert.transformer.layer.3.attention.v_lin.bias',\n",
       " 'distilbert.transformer.layer.3.attention.out_lin.weight',\n",
       " 'distilbert.transformer.layer.3.attention.out_lin.bias',\n",
       " 'distilbert.transformer.layer.3.sa_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.3.sa_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.3.ffn.lin1.weight',\n",
       " 'distilbert.transformer.layer.3.ffn.lin1.bias',\n",
       " 'distilbert.transformer.layer.3.ffn.lin2.weight',\n",
       " 'distilbert.transformer.layer.3.ffn.lin2.bias',\n",
       " 'distilbert.transformer.layer.3.output_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.3.output_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.4.attention.q_lin.weight',\n",
       " 'distilbert.transformer.layer.4.attention.q_lin.bias',\n",
       " 'distilbert.transformer.layer.4.attention.k_lin.weight',\n",
       " 'distilbert.transformer.layer.4.attention.k_lin.bias',\n",
       " 'distilbert.transformer.layer.4.attention.v_lin.weight',\n",
       " 'distilbert.transformer.layer.4.attention.v_lin.bias',\n",
       " 'distilbert.transformer.layer.4.attention.out_lin.weight',\n",
       " 'distilbert.transformer.layer.4.attention.out_lin.bias',\n",
       " 'distilbert.transformer.layer.4.sa_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.4.sa_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.4.ffn.lin1.weight',\n",
       " 'distilbert.transformer.layer.4.ffn.lin1.bias',\n",
       " 'distilbert.transformer.layer.4.ffn.lin2.weight',\n",
       " 'distilbert.transformer.layer.4.ffn.lin2.bias',\n",
       " 'distilbert.transformer.layer.4.output_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.4.output_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.5.attention.q_lin.weight',\n",
       " 'distilbert.transformer.layer.5.attention.q_lin.bias',\n",
       " 'distilbert.transformer.layer.5.attention.k_lin.weight',\n",
       " 'distilbert.transformer.layer.5.attention.k_lin.bias',\n",
       " 'distilbert.transformer.layer.5.attention.v_lin.weight',\n",
       " 'distilbert.transformer.layer.5.attention.v_lin.bias',\n",
       " 'distilbert.transformer.layer.5.attention.out_lin.weight',\n",
       " 'distilbert.transformer.layer.5.attention.out_lin.bias',\n",
       " 'distilbert.transformer.layer.5.sa_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.5.sa_layer_norm.bias',\n",
       " 'distilbert.transformer.layer.5.ffn.lin1.weight',\n",
       " 'distilbert.transformer.layer.5.ffn.lin1.bias',\n",
       " 'distilbert.transformer.layer.5.ffn.lin2.weight',\n",
       " 'distilbert.transformer.layer.5.ffn.lin2.bias',\n",
       " 'distilbert.transformer.layer.5.output_layer_norm.weight',\n",
       " 'distilbert.transformer.layer.5.output_layer_norm.bias',\n",
       " 'pre_classifier.weight',\n",
       " 'pre_classifier.bias',\n",
       " 'classifier.weight',\n",
       " 'classifier.bias']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n, p in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed_val = 111\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss after itaration 1: 0.165430\n",
      "Validation loss after itaration 1: 0.273891\n",
      "Time: 209m 26s\n",
      "\n",
      "Train loss after itaration 2: 0.122786\n",
      "Validation loss after itaration 2: 0.237248\n",
      "Time: 208m 49s\n",
      "\n",
      "Train loss after itaration 3: 0.104519\n",
      "Validation loss after itaration 3: 0.252435\n",
      "Time: 212m 42s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "num_mb_train = len(train_dataloader)\n",
    "num_mb_val = len(val_dataloader)\n",
    "\n",
    "if num_mb_val == 0:\n",
    "    num_mb_val = 1\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for k, (mb_x, mb_m, mb_y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        \n",
    "        mb_x = mb_x.to(device)\n",
    "        mb_m = mb_m.to(device)\n",
    "        mb_y = mb_y.to(device)\n",
    "        \n",
    "        outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        #loss = model_loss(outputs[1], mb_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.data / num_mb_train\n",
    "    \n",
    "    print (\"\\nTrain loss after itaration %i: %f\" % (n+1, train_loss))\n",
    "    train_losses.append(train_loss.cpu())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for k, (mb_x, mb_m, mb_y) in enumerate(val_dataloader):\n",
    "            mb_x = mb_x.to(device)\n",
    "            mb_m = mb_m.to(device)\n",
    "            mb_y = mb_y.to(device)\n",
    "        \n",
    "            outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            #loss = model_loss(outputs[1], mb_y)\n",
    "            \n",
    "            val_loss += loss.data / num_mb_val\n",
    "            \n",
    "        print (\"Validation loss after itaration %i: %f\" % (n+1, val_loss))\n",
    "        val_losses.append(val_loss.cpu())\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Time: {epoch_mins}m {epoch_secs}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x207817f3c48>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x207817e1ec8>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "test_data = TensorDataset(test_x, test_m)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for k, (mb_x, mb_m) in enumerate(test_dataloader):\n",
    "        mb_x = mb_x.to(device)\n",
    "        mb_m = mb_m.to(device)\n",
    "        output = model(mb_x, attention_mask=mb_m)\n",
    "        outputs.append(output[0].to('cpu'))\n",
    "\n",
    "outputs = torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted_values = torch.max(outputs, 1)\n",
    "predicted_values = predicted_values.numpy()\n",
    "true_values = test_y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9099210266535045\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = np.sum(predicted_values == true_values) / len(true_values)\n",
    "print (\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      3577\n",
      "           1       0.67      0.45      0.54       475\n",
      "\n",
      "    accuracy                           0.91      4052\n",
      "   macro avg       0.80      0.71      0.74      4052\n",
      "weighted avg       0.90      0.91      0.90      4052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_values, predicted_values, target_names=[str(l) for l in label_values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
